{"display_date": "2011/12/29 12:32:00 -0800", "views_count": 1481, "short_url": "http://post.ly/4djQS", "site_id": 75592, "site": {"subhead": "Data exploration and computer advice.", "full_hostname": "arsatiki.posterous.com", "name": "The Technical", "profile_image_75": "http://files.posterous.com/user_profile_pics/30986/che_baarissa_ilman_paitaa.png", "hostname": "arsatiki", "is_group": false, "time_zone": "Pacific Time (US & Canada)", "current_user_notification_frequency": "", "profile_image_35": "http://files.posterous.com/user_profile_pics/30986/che_baarissa_ilman_paitaa_thumb.png", "comment_permission": 2, "id": 75592, "sharing_enabled": false, "posts_count": 20, "header_image": null, "is_private": false, "current_user_role": ""}, "likes": [], "allowed": true, "body_full": "<p>At my last employer I looked briefly into&nbsp;<a href=\"http://en.wikipedia.org/wiki/A/B_testing\">A/B tests</a>. Many of the proposed solutions rely on classical statistical tests, such as the z-test or G-test. These tests are not very appealing to me. They solve an approximate problem exactly; I prefer the converse.</p>\r\n<p />\r\n<div>To my delight, I found a three-part series by Sir Evan Haas, which discussed using Bayesian inference on A/B tests. Sadly, only&nbsp;<a href=\"http://sirevanhaas.com/?p=30\">parts I</a>&nbsp;and&nbsp;<a href=\"http://sirevanhaas.com/?p=64\">part II</a>&nbsp;are available. The blog seems to have died out before part III ever came out.&nbsp;Part II ended with a cliffhanger: a hairy integral and a promise \"solved in Part III\".&nbsp;</div>\r\n<p />\r\n<div>For the last year and a half I've thought about the problem on and off. WolframAlpha could not solve the integral in a meaningful fashion. I plotted the joint probability distribution with R, but this left me wanting more.</div>\r\n<p />\r\n<div>A month ago I bumped into Bayesian networks (although I prefer \"probabilistic graphical models\"). This triggered a problem solving cascade in my mind. I finally managed to crack the problem! Both approximately and exactly! Oh happy times! Oh groovy times!</div>\r\n<p />\r\n<div><strong>A simple variant</strong></div>\r\n<p />\r\n<div>In his articles, Evan considers the whole joint probability distribution at once. That approach leads quickly into a mathematical briar patch with eye-poking indices and double integrals. However, the thorny mess becomes simpler with some theory.</div>\r\n<p />\r\n<div>So let us look at a simpler case&ndash;estimating the conversion rate for only one alternative. More precisely,&nbsp;what do we know about the conversion rate, if we've seen <em>s</em>&nbsp;conversions out of <em>n</em>&nbsp;trials? How is the probability distributed?</div>\r\n<p />\r\n<div>This is incidentally the oldest trick in the book. I mean this literally. The essay, in which Thomas Bayes debuted his formula, was dedicated to finding out the answer to this very question.&nbsp;<a href=\"http://www.stat.ucla.edu/history/essay.pdf\">The essay</a>&nbsp;is available on the Internet.</div>\r\n<p />\r\n<div>I will use different symbols than Evan does. Sorry about that. The unknown conversion rate is denoted by <em>r.</em>&nbsp;Our data consists of the number of successes&nbsp;<em>s</em>&nbsp;and the number of failures&nbsp;<em>f</em>&nbsp; in&nbsp;<em>n</em>&nbsp;trials. Note that <em>n</em>&nbsp;=<em>&nbsp;s</em>&nbsp;+ <em>f</em>.</div>\r\n<p />\r\n<div>Let's use the same binomial model as Evan for our likelihood. That is, we assume the number of successes<em>&nbsp;s</em>&nbsp;to be distributed according to the binomial distribution: <em>s ~ B</em>(<em>s </em>+<em> f, r</em>).</div>\r\n<p />\r\n<div>At this point I found the missing ingredient:&nbsp;<a href=\"http://en.wikipedia.org/wiki/Conjugate_prior\">conjugate priors</a>. A conjugate prior means that our posterior \"looks the same\" as the prior itself. Both the posterior and the prior belong to a \"same family.\" Their difference is usually in the parameters of the distribution. This allows algebraically nice updating rules for the posterior. In this case, the rules are very very pleasant.</div>\r\n<p />\r\n<div>The conjugate prior for the binomial distribution is the beta distribution Be(<em>&alpha;, &beta;</em>). Limiting our prior in this way seems foolish and constricting. What if there are no sane parameter choices for the prior? Fear not! The Be(1, 1) distribution is the same as the uniform distribution over (0, 1). It is a good choice for this application, where we do not know anything about the conversion rate.</div>\r\n<p />\r\n<div>The updating rule for the posterior is now exceedingly simple:</div>\r\n<p />\r\n<div style=\"text-align: center;\"><em>r</em> ~ Be(<em>s&nbsp;</em>+ 1, <em>f</em>&nbsp; + 1)</div>\r\n<p />\r\n<div>In general, if our prior is distributed according to Be(<em>&alpha;,&nbsp;&beta;</em>), then the posterior is</div>\r\n<p />\r\n<div style=\"text-align: center;\"><em>r</em> ~ Be(<em>&alpha;</em>&nbsp;+ <em>s,&nbsp;&beta;</em>&nbsp;+ <em>f</em>)</div>\r\n<p />\r\n<div>This is a wonderful result. We can mostly omit the Bayesian mechanism that led us here. Instead, we can simply use the beta distribution! (Turns out that people in medicine and other fields have known this result for ever.)</div>\r\n<p />\r\n<div><strong>Example time</strong></div>\r\n<p />\r\n<div>Let's assume we've seen 5 conversions out of 100 trials. This means that <em>s</em>&nbsp;= 5 and <em>f</em>&nbsp;= 100-5 = 95. You can use WolframAlpha for plotting.</div>\r\n<div><a href=\"http://www.wolframalpha.com/input/?i=BetaDistribution%5B6,+96%5D\"></a><a href=\"http://www.wolframalpha.com/input/?i=BetaDistribution%5B6,+96%5D\"><div class='p_embed p_image_embed'>\n<a href=\"http://getfile4.posterous.com/getfile/files.posterous.com/arsatiki/QhUgwSQjhm2PbPaSJQC07m3xPwJY71kkBVyK9LORBBILUasnrhETeTbjdK85/wolframalpha-20111203061804543.gif\"><img alt=\"Wolframalpha-20111203061804543\" height=\"167\" src=\"http://getfile5.posterous.com/getfile/files.posterous.com/arsatiki/UrEGTyuSjt7IAeUWUUwu8FWfKSW0cWJd9OGu1rKB9bjqc4eWJ10v1HvlXIkt/wolframalpha-20111203061804543.gif.scaled.500.jpg\" width=\"500\" /></a>\n</div>\n</a><a href=\"http://www.wolframalpha.com/input/?i=BetaDistribution%5B6%2C96%5D\">Link to full results</a>.</div>\r\n<p />\r\n<div>The true conversion rate seems to lie between 0 and 0.15. The mean is 0.056, which sounds reasonable.</div>\r\n<p />\r\n<div>If we have more data, our estimate improves. For 50 conversions and 1000 trials, we get the following PDF:</div>\r\n<div><div class='p_embed p_image_embed'>\n<a href=\"http://getfile1.posterous.com/getfile/files.posterous.com/arsatiki/BDZUStUH3f3ygaPAYZtv6Wg4yTUC4XrpepWRz6YW5dPNyR5Bi3FNrLutckX8/wolframalpha-20111203062445727.gif\"><img alt=\"Wolframalpha-20111203062445727\" height=\"162\" src=\"http://getfile2.posterous.com/getfile/files.posterous.com/arsatiki/DZj6juc7ogwtFsPy4zMhfGe18WlWrkbW81eeFsxkn1A7ioFDatQejkkztMBH/wolframalpha-20111203062445727.gif.scaled.500.jpg\" width=\"500\" /></a>\n</div>\n</div>\r\n<div>The probability mass is now very sharply centered around 0.05.</div>\r\n<p />\r\n<div><strong>Numerical solutions to the full problem</strong></div>\r\n<p />\r\n<div>Since we now have a handle what happens with a single variant, let's crack the full problem. Given the successes <em>s\u2081</em>, <em>s\u2082</em>&nbsp;and respective failures <em>f\u2081</em>, <em>f\u2082</em>&nbsp;for the two variants, what is the probability P(<em>r\u2081</em>&nbsp;&gt; <em>r</em>\u2082)? This is the 2D integral in Evan's Part II. (I'm not going to repeat it here.)</div>\r\n<p />\r\n<div>Originally I solved the question by integrating the joint distribution numerically over a 100 x 100 grid. If &nbsp;This gives rather good results. Sadly, I can not find the code I used.</div>\r\n<p />\r\n<div>After discovering that the distribution is a joint distribution of two independent beta distributions, I realized I can also use simulation. This is conceptually super simple.</div>\r\n<p />\r\n<div>First, generate <em>N</em>&nbsp;pairs of random samples from the joint distribution. Since they are independent, you need only pick the first item in the pair from the first beta distribution and similarly for the second item. You may need to generate a lot of samples, if the two rates are close to each other.</div>\r\n<p />\r\n<div>Count how many of the pairs have the first number greater than the second. Let us call this number <em>k</em>.</div>\r\n<p />\r\n<div>Then you can get the estimate for the probability P(<em>r\u2081</em>&nbsp;&gt; r\u2082)&nbsp;simply dividing <em>k</em>&nbsp;/ <em>N</em>.</div>\r\n<p />\r\n<div>Below is a simple implementation in Python:</div>\r\n<p />\r\n<div><a href=\"https://gist.github.com/1395348/6dccd38d608437ccd1a9a15aa856b326e86898ce\">https://gist.github.com/1395348/6dccd38d608437ccd1a9a15aa856b326e86898ce</a></div>\r\n<p />\r\n<div><strong>An exact solution! Hallelujah!</strong></div>\r\n<p />\r\n<div>First of all, don't thank me. Thank&nbsp;John D. Cook.&nbsp;<a href=\"http://www.johndcook.com/blog/2008/08/21/random-inequalities-v-beta-distributions/\">His article about random inequalities</a>&nbsp;contained the hint that when one of the parameters is an integer, we can compute a closed solution. One? <em>All of our parameters are integers!</em></div>\r\n<p />\r\n<div>My solution follows the mathematical form given in the&nbsp;<a href=\"http://www.mdanderson.org/education-and-research/departments-programs-and-labs/departments-and-divisions/division-of-quantitative-sciences/research/biostats-utmdabtr-005-05.pdf\">linked PDF article</a>. The code is below.</div>\r\n<p />\r\n<div><a href=\"https://gist.github.com/1395348/f0275f529d322d3c23e18201f26890f5a09dcb51\">https://gist.github.com/1395348/f0275f529d322d3c23e18201f26890f5a09dcb51</a></div>\r\n<p />\r\n<div>The code is a rather straightforward translation. I'd rather have used recursion, but the Python call stack blows up when <em>d</em>&nbsp;is anything significant.</div>\r\n<p />\r\n<div>Also note that my code is numerically insane (and slow). If you wanted to do this for a living, you'd better use the lgamma function to compute the logarithm of the gamma function. Gamma functions are in effect factorials. They grow very very fast; a float or even a double will overflow &nbsp;quickly.&nbsp;&nbsp;For example, the g0 function would be</div>\r\n<p />\r\n<div style=\"text-align: center;\">exp(lgamma(a+b) + lgamma(a+c) - lgamma(a+b+c) - lgamma(a)).</div>\r\n<p />\r\n<div>You'd also want to use the permutation tricks mentioned in the article to recurse on the smallest parameter.</div>\r\n<p />\r\n<div><strong>Summary</strong></div>\r\n<p />\r\n<div>\r\n<ul class=\"MailOutline\">\r\n<li>The distribution of the conversion rate for one variant is the beta distribution. (This applies when our model is the binomial model. More complex approaches are of course possible.)</li>\r\n<li>You can solve P(A &gt; B) it numerically (in two ways) or exactly.</li>\r\n<li>These are all known results.</li>\r\n</ul>\r\n</div>", "post_image_500": "http://getfile5.posterous.com/getfile/files.posterous.com/arsatiki/UrEGTyuSjt7IAeUWUUwu8FWfKSW0cWJd9OGu1rKB9bjqc4eWJ10v1HvlXIkt/wolframalpha-20111203061804543.gif.scaled.500.jpg", "id": 91157488, "twitter_account": "", "title": "Bayesian A/B testing with theory and code", "media": {"images": [{"full": {"username": "Antti", "caption": "Bayesian A/B testing with theory and code", "url": "http://getfile4.posterous.com/getfile/files.posterous.com/arsatiki/QhUgwSQjhm2PbPaSJQC07m3xPwJY71kkBVyK9LORBBILUasnrhETeTbjdK85/wolframalpha-20111203061804543.gif", "height": 197, "post_id": 91157488, "width": 590, "size": 7}, "media_id": "SjwFf9BAiRCfwMuMvzSK", "scaled500": {"username": "Antti", "caption": "Bayesian A/B testing with theory and code", "url": "http://getfile5.posterous.com/getfile/files.posterous.com/arsatiki/UrEGTyuSjt7IAeUWUUwu8FWfKSW0cWJd9OGu1rKB9bjqc4eWJ10v1HvlXIkt/wolframalpha-20111203061804543.gif.scaled.500.jpg", "height": 167, "post_id": 91157488, "width": 500, "size": 7}}, {"full": {"username": "Antti", "caption": "Bayesian A/B testing with theory and code", "url": "http://getfile1.posterous.com/getfile/files.posterous.com/arsatiki/BDZUStUH3f3ygaPAYZtv6Wg4yTUC4XrpepWRz6YW5dPNyR5Bi3FNrLutckX8/wolframalpha-20111203062445727.gif", "height": 191, "post_id": 91157488, "width": 590, "size": 7}, "media_id": "vfewcTQrWn8hbFIWyKY4", "scaled500": {"username": "Antti", "caption": "Bayesian A/B testing with theory and code", "url": "http://getfile2.posterous.com/getfile/files.posterous.com/arsatiki/DZj6juc7ogwtFsPy4zMhfGe18WlWrkbW81eeFsxkn1A7ioFDatQejkkztMBH/wolframalpha-20111203062445727.gif.scaled.500.jpg", "height": 162, "post_id": 91157488, "width": 500, "size": 7}}], "audio_files": [], "videos": []}, "comments": [{"body": "Also see http://www.quora.com/In-A-B-Testing-how-many-conversions-do-you-need-per-variation-for-the-results-to-be-significant\r\n\r\nIan Clarke proposes a probability weighed display of alternates: pick a number from the beta distribution and show the alternate which has the biggest number. The end result is that the alternates with good conversion rates are shown more than the ones with poor rates.", "name": null, "comment_type": "comment", "created_at": "2011/12/29 12:55:25 -0800", "post_id": 91157488, "user": {"body": "I am much more prolific in Twitter. See http://twitter.com/arsatiki\r\n", "profile_pic": "http://files.posterous.com/user_profile_pics/30986/che_baarissa_ilman_paitaa.png", "display_name": "Antti Rasinen", "firstname": "Antti", "lastname": "Rasinen", "profile_image_35": "http://files.posterous.com/user_profile_pics/30986/che_baarissa_ilman_paitaa_thumb.png", "last_activity": "2012/02/23 08:30:10 -0800", "nickname": "Antti", "id": 29785, "profile_url": "http://posterous.com/users/KPdfsrBQmB"}, "id": 10288854}, {"body": "Nice work.  I had actually written some ruby code to do the exact integration (using GMP rationals), but my laptop died before I had a chance to post it, and I sort of lost the motivation to do so.  It's still sitting on the hard drive...you may have just convinced me to dig out the old thing and finally post part 3.  I'll try to get it posted around the 2nd week of January.", "name": "SirEvanHaas", "comment_type": "comment", "created_at": "2011/12/29 15:42:20 -0800", "post_id": 91157488, "id": 10289329}, {"body": "Great post. You may find this other post by John D Cook interesting: http://www.johndcook.com/blog/2011/09/27/bayesian-amazon/ . He comes to the same conclusion as you but doesn't give as thorough an explanation.\r\n\r\nI'd be curious to see the relationship between the bayesian probabilities produced by your code and values coming out of the G-test for various (s_1, s_2, f_1, f_2) combinations. I'd wager there's a relationship where they approximate each other. ", "name": "snoble", "comment_type": "comment", "created_at": "2011/12/29 16:09:17 -0800", "post_id": 91157488, "id": 10289392}, {"body": "I'd like to highlight the other important point in Ian Clarke's presentation, namely, you can use other priors as well.\r\n\r\nAfter all, the prior is supposed to be \"everything you know before you saw the data\". If you know your conversion rates tend to be around 5%, Beta(6,96) is a sensible prior. And so is Beta(51,951), but with less variance. The nice thing is that a) it doesn't matter which prior you pick; in the long run data will dominate, and b) it's easy to estimate the effects of the prior vs data by thinking them as events that have already been seen.\r\n\r\nFinally, even though there is an accurate solution for the integer counts, simulation is also very nice. It's trivial to extend to new cases (A/B/C testing) and to ask different queries. Let's say you report to the boss \"The A option is very likely superior (p=99%), with a 5.7% conversion rate\", and the boss wants to know how reliable is the conversion rate estimate. Easy: compute 1000 samples, and report 25th and 975th as the 95% confidence interval.", "name": null, "comment_type": "comment", "created_at": "2011/12/30 05:05:04 -0800", "post_id": 91157488, "user": {"body": null, "profile_pic": "http://posterous.com/images/profile/missing-user-75.png", "display_name": "Lasse Rasinen", "firstname": "Lasse", "lastname": "Rasinen", "profile_image_35": "http://posterous.com/images/profile/missing-user-75.png", "last_activity": "2012/08/25 05:46:12 -0700", "nickname": "lrasinen", "id": 2107923, "profile_url": "http://posterous.com/users/cPuxiaUQYd1lo"}, "id": 10291110}, {"body": "Evan: What wonderful news! (Except the bit about the laptop dying, but dying is their nature.) I'd really love to see Part III. I don't think I am the only one either.\r\n\r\nToday I've had the most persistent bug of wanting to write a C version of the above code with all the possible optimizations I can think of. Let's see if the feeling passes =)", "name": null, "comment_type": "comment", "created_at": "2011/12/30 11:46:54 -0800", "post_id": 91157488, "user": {"body": "I am much more prolific in Twitter. See http://twitter.com/arsatiki\r\n", "profile_pic": "http://files.posterous.com/user_profile_pics/30986/che_baarissa_ilman_paitaa.png", "display_name": "Antti Rasinen", "firstname": "Antti", "lastname": "Rasinen", "profile_image_35": "http://files.posterous.com/user_profile_pics/30986/che_baarissa_ilman_paitaa_thumb.png", "last_activity": "2012/02/23 08:30:10 -0800", "nickname": "Antti", "id": 29785, "profile_url": "http://posterous.com/users/KPdfsrBQmB"}, "id": 10292258}, {"body": "snoble: Amazing. I read that article back in September\u2026 but since I didn't know about the role of the beta distribution I've never made the connection. Thank you for bringing it up!\r\n\r\nYour idea of tabulating the various tests is a good one. I'll start thinking about it. Sadly the 4d parameter space makes naive visualization impossible. Perhaps some sort of an interactive Javascript-enhanced tool.\r\n\r\nHonestly, with my current velocity it might take another 1,5 years to complete\u2026\r\n\r\nI'd assume the tests all work the same when there is a lot of data. After all, the binomial distribution looks like a Gaussian with large enough n. The interesting thing would be to map where the transition from \"reject\" to \"accept\" happens. Ponder ponder.", "name": null, "comment_type": "comment", "created_at": "2011/12/30 12:24:24 -0800", "post_id": 91157488, "user": {"body": "I am much more prolific in Twitter. See http://twitter.com/arsatiki\r\n", "profile_pic": "http://files.posterous.com/user_profile_pics/30986/che_baarissa_ilman_paitaa.png", "display_name": "Antti Rasinen", "firstname": "Antti", "lastname": "Rasinen", "profile_image_35": "http://files.posterous.com/user_profile_pics/30986/che_baarissa_ilman_paitaa_thumb.png", "last_activity": "2012/02/23 08:30:10 -0800", "nickname": "Antti", "id": 29785, "profile_url": "http://posterous.com/users/KPdfsrBQmB"}, "id": 10292356}, {"body": "Lasse: Good points. The beta distribution has an alternative parametrization, where you specify the mean and the sample size. That would be a handy way to describe a more informative prior: \"I'd expect the mean to be 5% with a prior sample size of 15\"\r\n\r\nOf course, then you'd most likely end up with non-integer parameters, which could impact the algorithm above.", "name": null, "comment_type": "comment", "created_at": "2011/12/30 12:29:22 -0800", "post_id": 91157488, "user": {"body": "I am much more prolific in Twitter. See http://twitter.com/arsatiki\r\n", "profile_pic": "http://files.posterous.com/user_profile_pics/30986/che_baarissa_ilman_paitaa.png", "display_name": "Antti Rasinen", "firstname": "Antti", "lastname": "Rasinen", "profile_image_35": "http://files.posterous.com/user_profile_pics/30986/che_baarissa_ilman_paitaa_thumb.png", "last_activity": "2012/02/23 08:30:10 -0800", "nickname": "Antti", "id": 29785, "profile_url": "http://posterous.com/users/KPdfsrBQmB"}, "id": 10292365}, {"body": "With the alpha-beta parameterization and a given r, you can get pretty close to any desired n, even if you fix one of the two to be an integer. \r\n\r\nI think the error for n is at most 0.5, but it's too close to 2012 for me to prove it :)", "name": null, "comment_type": "comment", "created_at": "2011/12/31 12:59:43 -0800", "post_id": 91157488, "user": {"body": null, "profile_pic": "http://posterous.com/images/profile/missing-user-75.png", "display_name": "Lasse Rasinen", "firstname": "Lasse", "lastname": "Rasinen", "profile_image_35": "http://posterous.com/images/profile/missing-user-75.png", "last_activity": "2012/08/25 05:46:12 -0700", "nickname": "lrasinen", "id": 2107923, "profile_url": "http://posterous.com/users/cPuxiaUQYd1lo"}, "id": 10295562}, {"body": "Great post - thanks! \r\nI've done a JavaScript implementation for visualizing two beta distributions together, here: http://www.peakconversion.com/2012/02/ab-split-test-graphical-calculator/. \r\nI\u2019d be really grateful for feedback from anyone here.\r\nI\u2019m aiming to add a feature for the simulation solution to the P(A>B) question very shortly.", "name": "jmgough", "comment_type": "comment", "created_at": "2012/02/23 04:03:15 -0800", "post_id": 91157488, "id": 10603913}, {"body": "jmgough: What a nice looking tool! jstat looks like a very useful piece of kit, too. I don't see any reason why it should be limited to just two alternates, though. You could easily plot three or four at the same time.\r\n\r\nMy main feedback isn't with the tool, but rather with the text. Humans are notoriously bad at eyeballing differences in distribution. The tool is great for visualization, but I wouldn't recommend it for decision making on its own. However, you did mention you'll be adding P(A > B), so that'll mitigate the issue. ", "name": null, "comment_type": "comment", "created_at": "2012/02/23 08:30:10 -0800", "post_id": 91157488, "user": {"body": "I am much more prolific in Twitter. See http://twitter.com/arsatiki\r\n", "profile_pic": "http://files.posterous.com/user_profile_pics/30986/che_baarissa_ilman_paitaa.png", "display_name": "Antti Rasinen", "firstname": "Antti", "lastname": "Rasinen", "profile_image_35": "http://files.posterous.com/user_profile_pics/30986/che_baarissa_ilman_paitaa_thumb.png", "last_activity": "2012/02/23 08:30:10 -0800", "nickname": "Antti", "id": 29785, "profile_url": "http://posterous.com/users/KPdfsrBQmB"}, "id": 10605367}, {"body": "Brilliant feedback Antti - really good points. I have made some pretty substantial changes.  Thanks!", "name": "jmgough", "comment_type": "comment", "created_at": "2012/02/26 14:27:37 -0800", "post_id": 91157488, "id": 10622712}], "body_cleaned": "<p>At my last employer I looked briefly into\u00a0<a href=\"http://en.wikipedia.org/wiki/A/B_testing\" rel=\"nofollow\">A/B tests</a>. Many of the proposed solutions rely on classical statistical tests, such as the z-test or G-test. These tests are not very appealing to me. They solve an approximate problem exactly; I prefer the converse.</p>\n<div>\u00a0</div>\n<div>To my delight, I found a three-part series by Sir Evan Haas, which discussed using Bayesian inference on A/B tests. Sadly, only\u00a0<a href=\"http://sirevanhaas.com/?p=30\" rel=\"nofollow\">parts I</a>\u00a0and\u00a0<a href=\"http://sirevanhaas.com/?p=64\" rel=\"nofollow\">part II</a>\u00a0are available. The blog seems to have died out before part III ever came out.\u00a0Part II ended with a cliffhanger: a hairy integral and a promise \"solved in Part III\".\u00a0</div>\n<div>\u00a0</div>\n<div>For the last year and a half I've thought about the problem on and off. WolframAlpha could not solve the integral in a meaningful fashion. I plotted the joint probability distribution with R, but this left me wanting more.</div>\n<div>\u00a0</div>\n<div>A month ago I bumped into Bayesian networks (although I prefer \"probabilistic graphical models\"). This triggered a problem solving cascade in my mind. I finally managed to crack the problem! Both approximately and exactly! Oh happy times! Oh groovy times!</div>\n<div>\u00a0</div>\n<div><strong>A simple variant</strong></div>\n<div>\u00a0</div>\n<div>In his articles, Evan considers the whole joint probability distribution at once. That approach leads quickly into a mathematical briar patch with eye-poking indices and double integrals. However, the thorny mess becomes simpler with some theory.</div>\n<div>\u00a0</div>\n<div>So let us look at a simpler case\u2013estimating the conversion rate for only one alternative. More precisely,\u00a0what do we know about the conversion rate, if we've seen <em>s</em>\u00a0conversions out of <em>n</em>\u00a0trials? How is the probability distributed?</div>\n<div>\u00a0</div>\n<div>This is incidentally the oldest trick in the book. I mean this literally. The essay, in which Thomas Bayes debuted his formula, was dedicated to finding out the answer to this very question.\u00a0<a href=\"http://www.stat.ucla.edu/history/essay.pdf\" rel=\"nofollow\">The essay</a>\u00a0is available on the Internet.</div>\n<div>\u00a0</div>\n<div>I will use different symbols than Evan does. Sorry about that. The unknown conversion rate is denoted by <em>r.</em>\u00a0Our data consists of the number of successes\u00a0<em>s</em>\u00a0and the number of failures\u00a0<em>f</em>\u00a0 in\u00a0<em>n</em>\u00a0trials. Note that <em>n</em>\u00a0=<em>\u00a0s</em>\u00a0+ <em>f</em>.</div>\n<div>\u00a0</div>\n<div>Let's use the same binomial model as Evan for our likelihood. That is, we assume the number of successes<em>\u00a0s</em>\u00a0to be distributed according to the binomial distribution: <em>s ~ B</em>(<em>s </em>+<em> f, r</em>).</div>\n<div>\u00a0</div>\n<div>At this point I found the missing ingredient:\u00a0<a href=\"http://en.wikipedia.org/wiki/Conjugate_prior\" rel=\"nofollow\">conjugate priors</a>. A conjugate prior means that our posterior \"looks the same\" as the prior itself. Both the posterior and the prior belong to a \"same family.\" Their difference is usually in the parameters of the distribution. This allows algebraically nice updating rules for the posterior. In this case, the rules are very very pleasant.</div>\n<div>\u00a0</div>\n<div>The conjugate prior for the binomial distribution is the beta distribution Be(<em>\u03b1, \u03b2</em>). Limiting our prior in this way seems foolish and constricting. What if there are no sane parameter choices for the prior? Fear not! The Be(1, 1) distribution is the same as the uniform distribution over (0, 1). It is a good choice for this application, where we do not know anything about the conversion rate.</div>\n<div>\u00a0</div>\n<div>The updating rule for the posterior is now exceedingly simple:</div>\n<div>\u00a0</div>\n<div>\n<em>r</em> ~ Be(<em>s\u00a0</em>+ 1, <em>f</em>\u00a0 + 1)</div>\n<div>\u00a0</div>\n<div>In general, if our prior is distributed according to Be(<em>\u03b1,\u00a0\u03b2</em>), then the posterior is</div>\n<div>\u00a0</div>\n<div>\n<em>r</em> ~ Be(<em>\u03b1</em>\u00a0+ <em>s,\u00a0\u03b2</em>\u00a0+ <em>f</em>)</div>\n<div>\u00a0</div>\n<div>This is a wonderful result. We can mostly omit the Bayesian mechanism that led us here. Instead, we can simply use the beta distribution! (Turns out that people in medicine and other fields have known this result for ever.)</div>\n<div>\u00a0</div>\n<div><strong>Example time</strong></div>\n<div>\u00a0</div>\n<div>Let's assume we've seen 5 conversions out of 100 trials. This means that <em>s</em>\u00a0= 5 and <em>f</em>\u00a0= 100-5 = 95. You can use WolframAlpha for plotting.</div>\n<div>\n<a href=\"http://www.wolframalpha.com/input/?i=BetaDistribution%5B6,+96%5D\" rel=\"nofollow\"><span data-type=\"image\" data-full-url=\"http://getfile5.posterous.com/getfile/files.posterous.com/arsatiki/UrEGTyuSjt7IAeUWUUwu8FWfKSW0cWJd9OGu1rKB9bjqc4eWJ10v1HvlXIkt/wolframalpha-20111203061804543.gif.scaled.500.jpg\" data-gallery-id=\"25570602\" data-thumb-url=\"http://getfile9.posterous.com/getfile/files.posterous.com/arsatiki/JxKMvd65tBQll3hArB45h4Cz9SEIUlUlK9AUWPLuP4QcIOH6WB8kA25sSkqI/wolframalpha-20111203061804543.gif.thumb100.jpg\" data-gallery-download=\"\" data-id=\"36128048\"></span></a><a href=\"http://www.wolframalpha.com/input/?i=BetaDistribution%5B6%2C96%5D\" rel=\"nofollow\">Link to full results</a>.</div>\n<div>\u00a0</div>\n<div>The true conversion rate seems to lie between 0 and 0.15. The mean is 0.056, which sounds reasonable.</div>\n<div>\u00a0</div>\n<div>If we have more data, our estimate improves. For 50 conversions and 1000 trials, we get the following PDF:</div>\n<div><span data-type=\"image\" data-full-url=\"http://getfile2.posterous.com/getfile/files.posterous.com/arsatiki/DZj6juc7ogwtFsPy4zMhfGe18WlWrkbW81eeFsxkn1A7ioFDatQejkkztMBH/wolframalpha-20111203062445727.gif.scaled.500.jpg\" data-gallery-id=\"25570603\" data-thumb-url=\"http://getfile5.posterous.com/getfile/files.posterous.com/arsatiki/MzvGwjqUs013DZi6O8qctcAPiI2KePrRzkdBn311ekaaiAHJzRGqTkSKztmO/wolframalpha-20111203062445727.gif.thumb100.jpg\" data-gallery-download=\"\" data-id=\"36128049\"></span></div>\n<div>The probability mass is now very sharply centered around 0.05.</div>\n<div>\u00a0</div>\n<div><strong>Numerical solutions to the full problem</strong></div>\n<div><strong><br /></strong></div>\n<div>Since we now have a handle what happens with a single variant, let's crack the full problem. Given the successes <em>s\u2081</em>, <em>s\u2082</em>\u00a0and respective failures <em>f\u2081</em>, <em>f\u2082</em>\u00a0for the two variants, what is the probability P(<em>r\u2081</em>\u00a0&gt; <em>r</em>\u2082)? This is the 2D integral in Evan's Part II. (I'm not going to repeat it here.)</div>\n<div>\u00a0</div>\n<div>Originally I solved the question by integrating the joint distribution numerically over a 100 x 100 grid. If \u00a0This gives rather good results. Sadly, I can not find the code I used.</div>\n<div>\u00a0</div>\n<div>After discovering that the distribution is a joint distribution of two independent beta distributions, I realized I can also use simulation. This is conceptually super simple.</div>\n<div>\u00a0</div>\n<div>First, generate <em>N</em>\u00a0pairs of random samples from the joint distribution. Since they are independent, you need only pick the first item in the pair from the first beta distribution and similarly for the second item. You may need to generate a lot of samples, if the two rates are close to each other.</div>\n<div>\u00a0</div>\n<div>Count how many of the pairs have the first number greater than the second. Let us call this number <em>k</em>.</div>\n<div>\u00a0</div>\n<div>Then you can get the estimate for the probability P(<em>r\u2081</em>\u00a0&gt; r\u2082)\u00a0simply dividing <em>k</em>\u00a0/ <em>N</em>.</div>\n<div>\u00a0</div>\n<div>Below is a simple implementation in Python:</div>\n<div>\u00a0</div>\n<div><a href=\"https://gist.github.com/1395348/6dccd38d608437ccd1a9a15aa856b326e86898ce\" rel=\"nofollow\">https://gist.github.com/1395348/6dccd38d608437ccd1a9a15aa856b326e86898ce</a></div>\n<div>\u00a0</div>\n<div><strong>An exact solution! Hallelujah!</strong></div>\n<div><strong><br /></strong></div>\n<div>First of all, don't thank me. Thank\u00a0John D. Cook.\u00a0<a href=\"http://www.johndcook.com/blog/2008/08/21/random-inequalities-v-beta-distributions/\" rel=\"nofollow\">His article about random inequalities</a>\u00a0contained the hint that when one of the parameters is an integer, we can compute a closed solution. One? <em>All of our parameters are integers!</em>\n</div>\n<div><em><br /></em></div>\n<div>My solution follows the mathematical form given in the\u00a0<a href=\"http://www.mdanderson.org/education-and-research/departments-programs-and-labs/departments-and-divisions/division-of-quantitative-sciences/research/biostats-utmdabtr-005-05.pdf\" rel=\"nofollow\">linked PDF article</a>. The code is below.</div>\n<div>\u00a0</div>\n<div><a href=\"https://gist.github.com/1395348/f0275f529d322d3c23e18201f26890f5a09dcb51\" rel=\"nofollow\">https://gist.github.com/1395348/f0275f529d322d3c23e18201f26890f5a09dcb51</a></div>\n<div>\u00a0</div>\n<div>The code is a rather straightforward translation. I'd rather have used recursion, but the Python call stack blows up when <em>d</em>\u00a0is anything significant.</div>\n<div>\u00a0</div>\n<div>Also note that my code is numerically insane (and slow). If you wanted to do this for a living, you'd better use the lgamma function to compute the logarithm of the gamma function. Gamma functions are in effect factorials. They grow very very fast; a float or even a double will overflow \u00a0quickly.\u00a0\u00a0For example, the g0 function would be</div>\n<div>\u00a0</div>\n<div>exp(lgamma(a+b) + lgamma(a+c) - lgamma(a+b+c) - lgamma(a)).</div>\n<div>\u00a0</div>\n<div>You'd also want to use the permutation tricks mentioned in the article to recurse on the smallest parameter.</div>\n<div>\u00a0</div>\n<div><strong>Summary</strong></div>\n<div><strong><br /></strong></div>\n<div>\n<ul>\n<li>The distribution of the conversion rate for one variant is the beta distribution. (This applies when our model is the binomial model. More complex approaches are of course possible.)</li>\n<li>You can solve P(A &gt; B) it numerically (in two ways) or exactly.</li>\n<li>These are all known results.</li>\n</ul>\n</div>", "draft": false, "full_url": "http://arsatiki.posterous.com/bayesian-ab-testing-with-theory-and-code", "scheduled": false, "tags": [], "body_html": "<p>At my last employer I looked briefly into&nbsp;<a href=\"http://en.wikipedia.org/wiki/A/B_testing\">A/B tests</a>. Many of the proposed solutions rely on classical statistical tests, such as the z-test or G-test. These tests are not very appealing to me. They solve an approximate problem exactly; I prefer the converse.</p>\r\n<p />\r\n<div>To my delight, I found a three-part series by Sir Evan Haas, which discussed using Bayesian inference on A/B tests. Sadly, only&nbsp;<a href=\"http://sirevanhaas.com/?p=30\">parts I</a>&nbsp;and&nbsp;<a href=\"http://sirevanhaas.com/?p=64\">part II</a>&nbsp;are available. The blog seems to have died out before part III ever came out.&nbsp;Part II ended with a cliffhanger: a hairy integral and a promise \"solved in Part III\".&nbsp;</div>\r\n<p />\r\n<div>For the last year and a half I've thought about the problem on and off. WolframAlpha could not solve the integral in a meaningful fashion. I plotted the joint probability distribution with R, but this left me wanting more.</div>\r\n<p />\r\n<div>A month ago I bumped into Bayesian networks (although I prefer \"probabilistic graphical models\"). This triggered a problem solving cascade in my mind. I finally managed to crack the problem! Both approximately and exactly! Oh happy times! Oh groovy times!</div>\r\n<p />\r\n<div><strong>A simple variant</strong></div>\r\n<p />\r\n<div>In his articles, Evan considers the whole joint probability distribution at once. That approach leads quickly into a mathematical briar patch with eye-poking indices and double integrals. However, the thorny mess becomes simpler with some theory.</div>\r\n<p />\r\n<div>So let us look at a simpler case&ndash;estimating the conversion rate for only one alternative. More precisely,&nbsp;what do we know about the conversion rate, if we've seen <em>s</em>&nbsp;conversions out of <em>n</em>&nbsp;trials? How is the probability distributed?</div>\r\n<p />\r\n<div>This is incidentally the oldest trick in the book. I mean this literally. The essay, in which Thomas Bayes debuted his formula, was dedicated to finding out the answer to this very question.&nbsp;<a href=\"http://www.stat.ucla.edu/history/essay.pdf\">The essay</a>&nbsp;is available on the Internet.</div>\r\n<p />\r\n<div>I will use different symbols than Evan does. Sorry about that. The unknown conversion rate is denoted by <em>r.</em>&nbsp;Our data consists of the number of successes&nbsp;<em>s</em>&nbsp;and the number of failures&nbsp;<em>f</em>&nbsp; in&nbsp;<em>n</em>&nbsp;trials. Note that <em>n</em>&nbsp;=<em>&nbsp;s</em>&nbsp;+ <em>f</em>.</div>\r\n<p />\r\n<div>Let's use the same binomial model as Evan for our likelihood. That is, we assume the number of successes<em>&nbsp;s</em>&nbsp;to be distributed according to the binomial distribution: <em>s ~ B</em>(<em>s </em>+<em> f, r</em>).</div>\r\n<p />\r\n<div>At this point I found the missing ingredient:&nbsp;<a href=\"http://en.wikipedia.org/wiki/Conjugate_prior\">conjugate priors</a>. A conjugate prior means that our posterior \"looks the same\" as the prior itself. Both the posterior and the prior belong to a \"same family.\" Their difference is usually in the parameters of the distribution. This allows algebraically nice updating rules for the posterior. In this case, the rules are very very pleasant.</div>\r\n<p />\r\n<div>The conjugate prior for the binomial distribution is the beta distribution Be(<em>&alpha;, &beta;</em>). Limiting our prior in this way seems foolish and constricting. What if there are no sane parameter choices for the prior? Fear not! The Be(1, 1) distribution is the same as the uniform distribution over (0, 1). It is a good choice for this application, where we do not know anything about the conversion rate.</div>\r\n<p />\r\n<div>The updating rule for the posterior is now exceedingly simple:</div>\r\n<p />\r\n<div style=\"text-align: center;\"><em>r</em> ~ Be(<em>s&nbsp;</em>+ 1, <em>f</em>&nbsp; + 1)</div>\r\n<p />\r\n<div>In general, if our prior is distributed according to Be(<em>&alpha;,&nbsp;&beta;</em>), then the posterior is</div>\r\n<p />\r\n<div style=\"text-align: center;\"><em>r</em> ~ Be(<em>&alpha;</em>&nbsp;+ <em>s,&nbsp;&beta;</em>&nbsp;+ <em>f</em>)</div>\r\n<p />\r\n<div>This is a wonderful result. We can mostly omit the Bayesian mechanism that led us here. Instead, we can simply use the beta distribution! (Turns out that people in medicine and other fields have known this result for ever.)</div>\r\n<p />\r\n<div><strong>Example time</strong></div>\r\n<p />\r\n<div>Let's assume we've seen 5 conversions out of 100 trials. This means that <em>s</em>&nbsp;= 5 and <em>f</em>&nbsp;= 100-5 = 95. You can use WolframAlpha for plotting.</div>\r\n<div><a href=\"http://www.wolframalpha.com/input/?i=BetaDistribution%5B6,+96%5D\"></a><a href=\"http://www.wolframalpha.com/input/?i=BetaDistribution%5B6,+96%5D\">[[posterous-content:SjwFf9BAiRCfwMuMvzSK]]</a><a href=\"http://www.wolframalpha.com/input/?i=BetaDistribution%5B6%2C96%5D\">Link to full results</a>.</div>\r\n<p />\r\n<div>The true conversion rate seems to lie between 0 and 0.15. The mean is 0.056, which sounds reasonable.</div>\r\n<p />\r\n<div>If we have more data, our estimate improves. For 50 conversions and 1000 trials, we get the following PDF:</div>\r\n<div>[[posterous-content:vfewcTQrWn8hbFIWyKY4]]</div>\r\n<div>The probability mass is now very sharply centered around 0.05.</div>\r\n<p />\r\n<div><strong>Numerical solutions to the full problem</strong></div>\r\n<p />\r\n<div>Since we now have a handle what happens with a single variant, let's crack the full problem. Given the successes <em>s\u2081</em>, <em>s\u2082</em>&nbsp;and respective failures <em>f\u2081</em>, <em>f\u2082</em>&nbsp;for the two variants, what is the probability P(<em>r\u2081</em>&nbsp;&gt; <em>r</em>\u2082)? This is the 2D integral in Evan's Part II. (I'm not going to repeat it here.)</div>\r\n<p />\r\n<div>Originally I solved the question by integrating the joint distribution numerically over a 100 x 100 grid. If &nbsp;This gives rather good results. Sadly, I can not find the code I used.</div>\r\n<p />\r\n<div>After discovering that the distribution is a joint distribution of two independent beta distributions, I realized I can also use simulation. This is conceptually super simple.</div>\r\n<p />\r\n<div>First, generate <em>N</em>&nbsp;pairs of random samples from the joint distribution. Since they are independent, you need only pick the first item in the pair from the first beta distribution and similarly for the second item. You may need to generate a lot of samples, if the two rates are close to each other.</div>\r\n<p />\r\n<div>Count how many of the pairs have the first number greater than the second. Let us call this number <em>k</em>.</div>\r\n<p />\r\n<div>Then you can get the estimate for the probability P(<em>r\u2081</em>&nbsp;&gt; r\u2082)&nbsp;simply dividing <em>k</em>&nbsp;/ <em>N</em>.</div>\r\n<p />\r\n<div>Below is a simple implementation in Python:</div>\r\n<p />\r\n<div><a href=\"https://gist.github.com/1395348/6dccd38d608437ccd1a9a15aa856b326e86898ce\">https://gist.github.com/1395348/6dccd38d608437ccd1a9a15aa856b326e86898ce</a></div>\r\n<p />\r\n<div><strong>An exact solution! Hallelujah!</strong></div>\r\n<p />\r\n<div>First of all, don't thank me. Thank&nbsp;John D. Cook.&nbsp;<a href=\"http://www.johndcook.com/blog/2008/08/21/random-inequalities-v-beta-distributions/\">His article about random inequalities</a>&nbsp;contained the hint that when one of the parameters is an integer, we can compute a closed solution. One? <em>All of our parameters are integers!</em></div>\r\n<p />\r\n<div>My solution follows the mathematical form given in the&nbsp;<a href=\"http://www.mdanderson.org/education-and-research/departments-programs-and-labs/departments-and-divisions/division-of-quantitative-sciences/research/biostats-utmdabtr-005-05.pdf\">linked PDF article</a>. The code is below.</div>\r\n<p />\r\n<div><a href=\"https://gist.github.com/1395348/f0275f529d322d3c23e18201f26890f5a09dcb51\">https://gist.github.com/1395348/f0275f529d322d3c23e18201f26890f5a09dcb51</a></div>\r\n<p />\r\n<div>The code is a rather straightforward translation. I'd rather have used recursion, but the Python call stack blows up when <em>d</em>&nbsp;is anything significant.</div>\r\n<p />\r\n<div>Also note that my code is numerically insane (and slow). If you wanted to do this for a living, you'd better use the lgamma function to compute the logarithm of the gamma function. Gamma functions are in effect factorials. They grow very very fast; a float or even a double will overflow &nbsp;quickly.&nbsp;&nbsp;For example, the g0 function would be</div>\r\n<p />\r\n<div style=\"text-align: center;\">exp(lgamma(a+b) + lgamma(a+c) - lgamma(a+b+c) - lgamma(a)).</div>\r\n<p />\r\n<div>You'd also want to use the permutation tricks mentioned in the article to recurse on the smallest parameter.</div>\r\n<p />\r\n<div><strong>Summary</strong></div>\r\n<p />\r\n<div>\r\n<ul class=\"MailOutline\">\r\n<li>The distribution of the conversion rate for one variant is the beta distribution. (This applies when our model is the binomial model. More complex approaches are of course possible.)</li>\r\n<li>You can solve P(A &gt; B) it numerically (in two ways) or exactly.</li>\r\n<li>These are all known results.</li>\r\n</ul>\r\n</div>", "locations": [], "user": {"body": "I am much more prolific in Twitter. See http://twitter.com/arsatiki\r\n", "profile_pic": "http://files.posterous.com/user_profile_pics/30986/che_baarissa_ilman_paitaa.png", "display_name": "Antti Rasinen", "firstname": "Antti", "lastname": "Rasinen", "profile_image_35": "http://files.posterous.com/user_profile_pics/30986/che_baarissa_ilman_paitaa_thumb.png", "last_activity": "2012/02/23 08:30:10 -0800", "nickname": "Antti", "id": 29785, "profile_url": "http://posterous.com/users/KPdfsrBQmB"}, "date": "2011-12-29 22:32:00+02:00", "replies_count": 0, "body_excerpt": "At my last employer I looked briefly into A/B tests. Many of the proposed solutions rely on classical statistical tests, such as the z-test or G-test. These tests are not very appealing to me. They solve an approximate problem exactly; I prefer th...", "slug": "bayesian-ab-testing-with-theory-and-code", "is_private": false, "likes_count": 0, "privatestring": "Euomhwmrys", "number_of_comments": 11, "comments_count": 11, "current_member": "", "author_display_name": "Antti Rasinen", "post_image_115": "http://getfile6.posterous.com/getfile/files.posterous.com/arsatiki/EX9AFnFL41euPERaELothjZEIkJat3F9K20rfkyLGNAJq3jr3GGaOZyCeRCA/wolframalpha-20111203061804543.gif.scaled.225.jpg", "is_owned_by_current_user": null}